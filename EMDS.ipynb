{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "702a381f-e31d-44bb-8802-dad7c0ffe6d3",
   "metadata": {},
   "source": [
    "# This script uses an API to retrieve storm report data from NOAA to assess the disaster caused by flash flooding in central Appalachia during July, 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "898dcefe-fd32-476f-86f1-7a9f2b8ed612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching file list from NCEI...\n",
      "Found 226 .csv.gz entries in directory listing.\n",
      "2022 details: downloading StormEvents_details-ftp_v1.0_d2022_c20250721.csv.gz\n",
      "Saved raw details gz: ./storm_events_download_study/raw/details/StormEvents_details-ftp_v1.0_d2022_c20250721.csv.gz\n",
      "2022 locations: downloading StormEvents_locations-ftp_v1.0_d2022_c20250721.csv.gz\n",
      "Saved raw locations gz: ./storm_events_download_study/raw/locations/StormEvents_locations-ftp_v1.0_d2022_c20250721.csv.gz\n",
      "\n",
      "[OK] Filtered details rows: 177\n",
      "[OK] Filtered locations rows: 277\n",
      "[OK] Wrote: ./storm_events_download_study/filtered/noaa_stormevents_2022_20220726_20220728_details.csv\n",
      "[OK] Wrote: ./storm_events_download_study/filtered/noaa_stormevents_2022_20220726_20220728_locations.csv\n",
      "[OK] Wrote: ./storm_events_download_study/filtered/noaa_stormevents_2022_20220726_20220728_details_locations_merged.csv\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import gzip\n",
    "import io\n",
    "from datetime import datetime\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "BASE_URL = \"https://www.ncei.noaa.gov/pub/data/swdi/stormevents/csvfiles/\"\n",
    "OUT_DIR = \"./storm_events_download_study\"\n",
    "\n",
    "UA = {\n",
    "    \"User-Agent\": \"stormevents-research/1.0 (contact: your_email@example.com)\"\n",
    "}\n",
    "\n",
    "C_RE = re.compile(r\"_c(\\d{8})\\.csv\\.gz$\", re.IGNORECASE)\n",
    "GZ_RE = re.compile(r\"\\.csv\\.gz$\", re.IGNORECASE)\n",
    "\n",
    "YEAR = 2022\n",
    "DATE_START = \"2022-07-26\"\n",
    "DATE_END   = \"2022-07-28\"\n",
    "\n",
    "#  flood-related hazards.\n",
    "KEEP_EVENT_TYPES = {\"Flash Flood\", \"Flood\", \"Heavy Rain\"}  # or None\n",
    "\n",
    "DETAILS_TIME_COL = \"BEGIN_DATE_TIME\"\n",
    "\n",
    "MERGE_KEY = \"EVENT_ID\"\n",
    "\n",
    "\n",
    "def ensure_dir(p):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "    return p\n",
    "\n",
    "\n",
    "def fetch_with_retries(url, headers, timeout=120, max_tries=5, backoff=2.0):\n",
    "    last_err = None\n",
    "    for i in range(1, max_tries + 1):\n",
    "        try:\n",
    "            r = requests.get(url, headers=headers, timeout=timeout)\n",
    "            r.raise_for_status()\n",
    "            return r.content\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            sleep_s = backoff ** (i - 1)\n",
    "            print(\"  download failed (try {0}/{1}): {2} | sleeping {3:.1f}s\".format(\n",
    "                i, max_tries, e, sleep_s\n",
    "            ))\n",
    "            time.sleep(sleep_s)\n",
    "    raise RuntimeError(\"Failed to download after {0} tries: {1}\".format(max_tries, url)) from last_err\n",
    "\n",
    "\n",
    "def list_remote_files():\n",
    "    idx = requests.get(BASE_URL, headers=UA, timeout=60)\n",
    "    idx.raise_for_status()\n",
    "    soup = BeautifulSoup(idx.text, \"html.parser\")\n",
    "    links = [a.get(\"href\") for a in soup.find_all(\"a\") if a.get(\"href\")]\n",
    "    return [l for l in links if l and GZ_RE.search(l)]\n",
    "\n",
    "\n",
    "def pick_latest_by_cdate(files):\n",
    "    candidates = []\n",
    "    no_c = []\n",
    "    for f in files:\n",
    "        m = C_RE.search(f)\n",
    "        if m:\n",
    "            candidates.append((datetime.strptime(m.group(1), \"%Y%m%d\"), f))\n",
    "        else:\n",
    "            no_c.append(f)\n",
    "\n",
    "    if candidates:\n",
    "        return max(candidates, key=lambda x: x[0])[1]\n",
    "    if no_c:\n",
    "        return sorted(no_c)[-1]\n",
    "    return None\n",
    "\n",
    "\n",
    "def read_gz_csv_bytes(content_bytes):\n",
    "    with gzip.GzipFile(fileobj=io.BytesIO(content_bytes)) as gz:\n",
    "        return pd.read_csv(gz, low_memory=False)\n",
    "\n",
    "\n",
    "def download_one_year(year, links, kind=\"details\"):\n",
    "    if kind == \"details\":\n",
    "        prefix = \"StormEvents_details-ftp_v1.0_d{0}\".format(year)\n",
    "    elif kind == \"locations\":\n",
    "        prefix = \"StormEvents_locations-ftp_v1.0_d{0}\".format(year)\n",
    "    else:\n",
    "        raise ValueError(\"kind must be details or locations\")\n",
    "\n",
    "    matches = [l for l in links if l.startswith(prefix) and l.endswith(\".csv.gz\")]\n",
    "    chosen = pick_latest_by_cdate(matches)\n",
    "    if not chosen:\n",
    "        return None, None, None\n",
    "\n",
    "    url = BASE_URL + chosen\n",
    "    print(\"{0} {1}: downloading {2}\".format(year, kind, chosen))\n",
    "    content = fetch_with_retries(url, UA, timeout=180, max_tries=6, backoff=2.0)\n",
    "\n",
    "    df = read_gz_csv_bytes(content)\n",
    "    return chosen, df, content\n",
    "\n",
    "\n",
    "def parse_noaa_dt(series):\n",
    "    s = series.astype(str).str.strip()\n",
    "    s = s.replace({\"\": pd.NA, \"nan\": pd.NA, \"NaT\": pd.NA})\n",
    "\n",
    "    dt = pd.to_datetime(s, errors=\"coerce\", format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    if dt.notna().any():\n",
    "        return dt\n",
    "    dt2 = pd.to_datetime(s, errors=\"coerce\", format=\"%d-%b-%y %H:%M:%S\")\n",
    "    if dt2.notna().any():\n",
    "        return dt2\n",
    "    dt3 = pd.to_datetime(s, errors=\"coerce\", format=\"%d-%b-%y %I:%M:%S %p\")\n",
    "    if dt3.notna().any():\n",
    "        return dt3\n",
    "    return pd.to_datetime(s, errors=\"coerce\")\n",
    "\n",
    "\n",
    "def filter_details_to_period(df_details):\n",
    "    if DETAILS_TIME_COL not in df_details.columns:\n",
    "        raise ValueError(\"Time column not found in details: {0}\".format(DETAILS_TIME_COL))\n",
    "\n",
    "    d = df_details.copy()\n",
    "    d[\"_dt\"] = parse_noaa_dt(d[DETAILS_TIME_COL])\n",
    "\n",
    "    start_dt = pd.to_datetime(DATE_START + \" 00:00:00\")\n",
    "    end_dt = pd.to_datetime(DATE_END + \" 23:59:59\")\n",
    "\n",
    "    d = d[d[\"_dt\"].between(start_dt, end_dt, inclusive=\"both\")].copy()\n",
    "\n",
    "    if KEEP_EVENT_TYPES is not None and \"EVENT_TYPE\" in d.columns:\n",
    "        d = d[d[\"EVENT_TYPE\"].astype(str).str.strip().isin(KEEP_EVENT_TYPES)].copy()\n",
    "\n",
    "    d.drop(columns=[\"_dt\"], inplace=True, errors=\"ignore\")\n",
    "    return d\n",
    "\n",
    "\n",
    "def filter_locations_by_event_ids(df_locations, event_ids):\n",
    "    if MERGE_KEY not in df_locations.columns:\n",
    "        raise ValueError(\"Merge key not found in locations: {0}\".format(MERGE_KEY))\n",
    "    return df_locations[df_locations[MERGE_KEY].isin(event_ids)].copy()\n",
    "\n",
    "\n",
    "def main():\n",
    "    raw_dir = ensure_dir(os.path.join(OUT_DIR, \"raw\"))\n",
    "    raw_det_dir = ensure_dir(os.path.join(raw_dir, \"details\"))\n",
    "    raw_loc_dir = ensure_dir(os.path.join(raw_dir, \"locations\"))\n",
    "    filt_dir = ensure_dir(os.path.join(OUT_DIR, \"filtered\"))\n",
    "\n",
    "    print(\"Fetching file list from NCEI...\")\n",
    "    links = list_remote_files()\n",
    "    print(\"Found {0} .csv.gz entries in directory listing.\".format(len(links)))\n",
    "\n",
    "    chosen_det, df_det, raw_det = download_one_year(YEAR, links, kind=\"details\")\n",
    "    if df_det is None:\n",
    "        raise RuntimeError(\"Details file not found for year {0}\".format(YEAR))\n",
    "\n",
    "    det_gz_path = os.path.join(raw_det_dir, chosen_det)\n",
    "    with open(det_gz_path, \"wb\") as f:\n",
    "        f.write(raw_det)\n",
    "    print(\"Saved raw details gz:\", det_gz_path)\n",
    "\n",
    "    chosen_loc, df_loc, raw_loc = download_one_year(YEAR, links, kind=\"locations\")\n",
    "    if df_loc is None:\n",
    "        raise RuntimeError(\"Locations file not found for year {0}\".format(YEAR))\n",
    "\n",
    "    loc_gz_path = os.path.join(raw_loc_dir, chosen_loc)\n",
    "    with open(loc_gz_path, \"wb\") as f:\n",
    "        f.write(raw_loc)\n",
    "    print(\"Saved raw locations gz:\", loc_gz_path)\n",
    "\n",
    "    df_det_f = filter_details_to_period(df_det)\n",
    "\n",
    "    if MERGE_KEY not in df_det_f.columns:\n",
    "        raise ValueError(\"Merge key not found in filtered details: {0}\".format(MERGE_KEY))\n",
    "\n",
    "    event_ids = df_det_f[MERGE_KEY].dropna().unique().tolist()\n",
    "    df_loc_f = filter_locations_by_event_ids(df_loc, event_ids)\n",
    "\n",
    "    tag = \"noaa_stormevents_{0}_{1}_{2}\".format(\n",
    "        YEAR,\n",
    "        DATE_START.replace(\"-\", \"\"),\n",
    "        DATE_END.replace(\"-\", \"\")\n",
    "    )\n",
    "\n",
    "    out_det = os.path.join(filt_dir, tag + \"_details.csv\")\n",
    "    out_loc = os.path.join(filt_dir, tag + \"_locations.csv\")\n",
    "    out_mrg = os.path.join(filt_dir, tag + \"_details_locations_merged.csv\")\n",
    "\n",
    "    df_det_f.to_csv(out_det, index=False)\n",
    "    df_loc_f.to_csv(out_loc, index=False)\n",
    "\n",
    "    # Merging all data\n",
    "    df_merged = df_loc_f.merge(df_det_f, on=MERGE_KEY, how=\"left\")\n",
    "    df_merged.to_csv(out_mrg, index=False)\n",
    "\n",
    "    print(\"\\n[OK] Filtered details rows:\", len(df_det_f))\n",
    "    print(\"[OK] Filtered locations rows:\", len(df_loc_f))\n",
    "    print(\"[OK] Wrote:\", out_det)\n",
    "    print(\"[OK] Wrote:\", out_loc)\n",
    "    print(\"[OK] Wrote:\", out_mrg)\n",
    "    print(\"\\nDone.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d81857-9b7e-44a1-9083-dc7562fc99e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5af5b60f-87a2-483d-a789-4ca0cb316b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded: ./storm_events_download_study/filtered/noaa_stormevents_2022_20220726_20220728_details.csv rows= 177\n",
      "[INFO] dt parsed non-null: 177\n",
      "[INFO] event types (top):\n",
      "event_type\n",
      "Flash Flood    145\n",
      "Flood           17\n",
      "Heavy Rain      15\n",
      "Name: count, dtype: int64\n",
      "[DEBUG] States after state filter (top):\n",
      "state\n",
      "KENTUCKY         38\n",
      "WEST VIRGINIA    21\n",
      "VIRGINIA         12\n",
      "Name: count, dtype: int64\n",
      "[INFO] after state+bbox filter: 71\n",
      "[DEBUG] Lat range: 36.7751 38.1121\n",
      "[DEBUG] Lon range: -84.4713 -80.24\n",
      "[OK] wrote: ./outputs_appalachia_202207/studyperiod_summary_table.csv\n",
      "[OK] saved: ./outputs_appalachia_202207/map_studyperiod_points_by_type.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/derecho/scratch/npinkrah/tmp/ipykernel_133640/2634277672.py:204: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  tmp[\"hour\"] = tmp[\"dt\"].dt.floor(\"H\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] saved: ./outputs_appalachia_202207/timeseries_hourly_counts.png\n",
      "[OK] saved: ./outputs_appalachia_202207/heatmap_hour_by_day.png\n",
      "[INFO] county choropleth skipped (set COUNTIES_SHP to enable).\n",
      "[DONE] All plots written to: ./outputs_appalachia_202207\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "EVENTS_CSV = \"./storm_events_download_study/filtered/noaa_stormevents_2022_20220726_20220728_details.csv\"\n",
    "STATES_SHP = \"./ne_50m_admin_1_states_provinces_lakes/ne_50m_admin_1_states_provinces_lakes.shp\"\n",
    "\n",
    "COUNTIES_SHP = None\n",
    "\n",
    "FOCUS_STATES = {\"KENTUCKY\", \"WEST VIRGINIA\", \"VIRGINIA\", \"TENNESSEE\", \"OHIO\"}\n",
    "\n",
    "BBOX = dict(lon_min=-91.0, lon_max=-79.0, lat_min=34.0, lat_max=41.5)\n",
    "\n",
    "OUT_DIR = \"./outputs_appalachia_202207\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "OUT_MAP = os.path.join(OUT_DIR, \"map_studyperiod_points_by_type.png\")\n",
    "OUT_HOURLY = os.path.join(OUT_DIR, \"timeseries_hourly_counts.png\")\n",
    "OUT_HEATMAP = os.path.join(OUT_DIR, \"heatmap_hour_by_day.png\")\n",
    "OUT_SUMMARY = os.path.join(OUT_DIR, \"studyperiod_summary_table.csv\")\n",
    "OUT_COUNTY = os.path.join(OUT_DIR, \"county_choropleth_counts.png\")\n",
    "\n",
    "\n",
    "def parse_noaa_dt(series):\n",
    "    s = series.astype(str).str.strip()\n",
    "    s = s.replace({\"\": pd.NA, \"nan\": pd.NA, \"NaT\": pd.NA})\n",
    "\n",
    "    dt = pd.to_datetime(s, errors=\"coerce\", format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    if dt.notna().any():\n",
    "        return dt\n",
    "    dt2 = pd.to_datetime(s, errors=\"coerce\", format=\"%d-%b-%y %H:%M:%S\")\n",
    "    if dt2.notna().any():\n",
    "        return dt2\n",
    "    dt3 = pd.to_datetime(s, errors=\"coerce\", format=\"%d-%b-%y %I:%M:%S %p\")\n",
    "    if dt3.notna().any():\n",
    "        return dt3\n",
    "\n",
    "    return pd.to_datetime(s, errors=\"coerce\")\n",
    "\n",
    "\n",
    "def find_first_col(df, candidates):\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    for cand in candidates:\n",
    "        k = cand.lower()\n",
    "        if k in cols:\n",
    "            return cols[k]\n",
    "    return None\n",
    "\n",
    "\n",
    "def main():\n",
    "    df = pd.read_csv(EVENTS_CSV, low_memory=False)\n",
    "    print(\"[INFO] Loaded:\", EVENTS_CSV, \"rows=\", len(df))\n",
    "\n",
    "    lat_col = find_first_col(df, [\"LATITUDE\", \"BEGIN_LAT\", \"LAT\"])\n",
    "    lon_col = find_first_col(df, [\"LONGITUDE\", \"BEGIN_LON\", \"LON\"])\n",
    "    time_col = find_first_col(df, [\"BEGIN_DATE_TIME\", \"BEGIN_DATETIME\", \"START_TIME\"])\n",
    "    etype_col = find_first_col(df, [\"EVENT_TYPE\"])\n",
    "    state_col = find_first_col(df, [\"STATE\"])\n",
    "    county_col = find_first_col(df, [\"CZ_NAME\", \"COUNTY\"])\n",
    "\n",
    "    if lat_col is None or lon_col is None:\n",
    "        raise ValueError(\"Could not find lat/lon columns in merged file.\")\n",
    "    if time_col is None:\n",
    "        raise ValueError(\"Could not find BEGIN_DATE_TIME (or equivalent) in merged file.\")\n",
    "    if etype_col is None:\n",
    "        print(\"[WARN] EVENT_TYPE not found; map legend may be limited.\")\n",
    "    if state_col is None:\n",
    "        print(\"[WARN] STATE not found; state filtering will be skipped.\")\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"dt\"] = parse_noaa_dt(df[time_col])\n",
    "\n",
    "    df[\"event_type\"] = df[etype_col].astype(str).str.strip() if etype_col else \"Unknown\"\n",
    "\n",
    "    # FIX: normalize STATE to uppercase to match FOCUS_STATES\n",
    "    if state_col:\n",
    "        df[\"state\"] = df[state_col].astype(str).str.strip().str.upper()\n",
    "    else:\n",
    "        df[\"state\"] = \"\"\n",
    "\n",
    "    if county_col:\n",
    "        df[\"county\"] = df[county_col].astype(str).str.strip()\n",
    "    else:\n",
    "        df[\"county\"] = \"\"\n",
    "\n",
    "    print(\"[INFO] dt parsed non-null:\", int(df[\"dt\"].notna().sum()))\n",
    "    print(\"[INFO] event types (top):\")\n",
    "    print(df[\"event_type\"].value_counts().head(10))\n",
    "\n",
    "    dff = df.copy()\n",
    "\n",
    "    if state_col:\n",
    "        dff = dff[dff[\"state\"].isin(FOCUS_STATES)].copy()\n",
    "        print(\"[DEBUG] States after state filter (top):\")\n",
    "        print(dff[\"state\"].value_counts().head(10))\n",
    "    else:\n",
    "        print(\"[WARN] Skipping state filter (STATE column missing).\")\n",
    "\n",
    "    dff = dff[\n",
    "        (dff[lon_col] >= BBOX[\"lon_min\"]) & (dff[lon_col] <= BBOX[\"lon_max\"]) &\n",
    "        (dff[lat_col] >= BBOX[\"lat_min\"]) & (dff[lat_col] <= BBOX[\"lat_max\"])\n",
    "    ].copy()\n",
    "\n",
    "    print(\"[INFO] after state+bbox filter:\", len(dff))\n",
    "    if len(dff) > 0:\n",
    "        print(\"[DEBUG] Lat range:\", float(dff[lat_col].min()), float(dff[lat_col].max()))\n",
    "        print(\"[DEBUG] Lon range:\", float(dff[lon_col].min()), float(dff[lon_col].max()))\n",
    "\n",
    "    # GeoDataFrame\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        dff,\n",
    "        geometry=gpd.points_from_xy(dff[lon_col], dff[lat_col]),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "\n",
    "    states = gpd.read_file(STATES_SHP).to_crs(\"EPSG:4326\")\n",
    "\n",
    "    name_col = find_first_col(states, [\"name\", \"name_en\", \"name_long\", \"admin\"]) or states.columns[0]\n",
    "\n",
    "    states[\"_name_u\"] = states[name_col].astype(str).str.strip().str.upper()\n",
    "\n",
    "    states_focus = states[states[\"_name_u\"].isin(FOCUS_STATES)].copy()\n",
    "    if len(states_focus) == 0:\n",
    "        print(\"[WARN] Could not match focus states in shapefile by name column:\", name_col)\n",
    "        states_focus = states.copy()\n",
    "\n",
    "    summary = {}\n",
    "    summary[\"n_points\"] = int(len(gdf))\n",
    "    summary[\"start_time\"] = str(gdf[\"dt\"].min()) if gdf[\"dt\"].notna().any() else \"\"\n",
    "    summary[\"end_time\"] = str(gdf[\"dt\"].max()) if gdf[\"dt\"].notna().any() else \"\"\n",
    "\n",
    "    vc = gdf[\"event_type\"].value_counts()\n",
    "    for k in vc.index[:10]:\n",
    "        summary[\"count_\" + k.replace(\" \", \"_\")] = int(vc.loc[k])\n",
    "\n",
    "    pd.DataFrame([summary]).to_csv(OUT_SUMMARY, index=False)\n",
    "    print(\"[OK] wrote:\", OUT_SUMMARY)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    states_focus.boundary.plot(ax=ax, linewidth=1)\n",
    "\n",
    "    if len(gdf) == 0:\n",
    "        ax.text(\n",
    "            0.5, 0.5,\n",
    "            \"No points after filters\\nCheck state names + BBOX\",\n",
    "            transform=ax.transAxes, ha=\"center\", va=\"center\"\n",
    "        )\n",
    "    else:\n",
    "        for etype, sub in gdf.groupby(\"event_type\"):\n",
    "            sub.plot(ax=ax, markersize=18, alpha=0.75, label=etype)\n",
    "        ax.legend(loc=\"upper right\", frameon=True, fontsize=9)\n",
    "\n",
    "    ax.set_xlim(BBOX[\"lon_min\"], BBOX[\"lon_max\"])\n",
    "    ax.set_ylim(BBOX[\"lat_min\"], BBOX[\"lat_max\"])\n",
    "    ax.set_xlabel(\"Longitude\")\n",
    "    ax.set_ylabel(\"Latitude\")\n",
    "    ax.set_title(\"NOAA Storm Event Reports (Study Period)\\nCentral Appalachia - July 26-27, 2022\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT_MAP, dpi=300)\n",
    "    plt.close(fig)\n",
    "    print(\"[OK] saved:\", OUT_MAP)\n",
    "\n",
    "\n",
    "    if len(gdf) > 0 and gdf[\"dt\"].notna().any():\n",
    "        tmp = gdf.copy()\n",
    "        tmp[\"hour\"] = tmp[\"dt\"].dt.floor(\"H\")\n",
    "        hourly = tmp.groupby(\"hour\").size().sort_index()\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 4))\n",
    "        ax.plot(hourly.index, hourly.values, marker=\"o\")\n",
    "        ax.set_title(\"Hourly Count of NOAA Reports (Study Period)\")\n",
    "        ax.set_xlabel(\"Time\")\n",
    "        ax.set_ylabel(\"Count\")\n",
    "        ax.grid(True, linewidth=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(OUT_HOURLY, dpi=300)\n",
    "        plt.close(fig)\n",
    "        print(\"[OK] saved:\", OUT_HOURLY)\n",
    "    else:\n",
    "        print(\"[WARN] Skipping hourly time series (no datetime or no points).\")\n",
    "\n",
    "    if len(gdf) > 0 and gdf[\"dt\"].notna().any():\n",
    "        tmp = gdf.copy()\n",
    "        tmp[\"day\"] = tmp[\"dt\"].dt.date.astype(str)\n",
    "        tmp[\"hour_of_day\"] = tmp[\"dt\"].dt.hour\n",
    "\n",
    "        value_col = \"EVENT_ID\" if \"EVENT_ID\" in tmp.columns else None\n",
    "        if value_col is None:\n",
    "            tmp[\"_one\"] = 1\n",
    "            value_col = \"_one\"\n",
    "\n",
    "        pivot = tmp.pivot_table(\n",
    "            index=\"hour_of_day\",\n",
    "            columns=\"day\",\n",
    "            values=value_col,\n",
    "            aggfunc=\"count\",\n",
    "            fill_value=0\n",
    "        )\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(9, 5))\n",
    "        im = ax.imshow(pivot.values, aspect=\"auto\", origin=\"lower\")\n",
    "        ax.set_title(\"Report Density Heatmap (Hour x Day)\")\n",
    "        ax.set_xlabel(\"Day\")\n",
    "        ax.set_ylabel(\"Hour of day\")\n",
    "\n",
    "        ax.set_xticks(range(len(pivot.columns)))\n",
    "        ax.set_xticklabels(pivot.columns, rotation=45, ha=\"right\")\n",
    "        ax.set_yticks(range(0, 24, 3))\n",
    "        ax.set_yticklabels([str(h) for h in range(0, 24, 3)])\n",
    "\n",
    "        fig.colorbar(im, ax=ax, label=\"Count\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(OUT_HEATMAP, dpi=300)\n",
    "        plt.close(fig)\n",
    "        print(\"[OK] saved:\", OUT_HEATMAP)\n",
    "    else:\n",
    "        print(\"[WARN] Skipping heatmap (no datetime or no points).\")\n",
    "\n",
    "    if COUNTIES_SHP is not None and os.path.exists(COUNTIES_SHP) and len(gdf) > 0 and county_col is not None:\n",
    "        counties = gpd.read_file(COUNTIES_SHP).to_crs(\"EPSG:4326\")\n",
    "        c_name = find_first_col(counties, [\"name\", \"name_en\", \"name_long\"]) or counties.columns[0]\n",
    "\n",
    "        counts = gdf.groupby(\"county\").size().reset_index(name=\"n_reports\")\n",
    "        counts[\"key\"] = counts[\"county\"].astype(str).str.lower()\n",
    "        counties[\"key\"] = counties[c_name].astype(str).str.lower()\n",
    "\n",
    "        merged = counties.merge(counts, on=\"key\", how=\"left\")\n",
    "        merged[\"n_reports\"] = merged[\"n_reports\"].fillna(0)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 7))\n",
    "        merged.plot(column=\"n_reports\", ax=ax, legend=True)\n",
    "        states_focus.boundary.plot(ax=ax, linewidth=0.8)\n",
    "\n",
    "        ax.set_xlim(BBOX[\"lon_min\"], BBOX[\"lon_max\"])\n",
    "        ax.set_ylim(BBOX[\"lat_min\"], BBOX[\"lat_max\"])\n",
    "        ax.set_title(\"County-level Report Counts (Best-effort)\")\n",
    "        ax.set_axis_off()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(OUT_COUNTY, dpi=300)\n",
    "        plt.close(fig)\n",
    "        print(\"[OK] saved:\", OUT_COUNTY)\n",
    "    else:\n",
    "        print(\"[INFO] county choropleth skipped (set COUNTIES_SHP to enable).\")\n",
    "\n",
    "    print(\"[DONE] All plots written to:\", OUT_DIR)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91efa914-4b43-4b44-8ad7-7e2e0d020752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================\n",
      "FILE: ./storm_events_download_study/filtered/noaa_stormevents_2022_20220726_20220728_details.csv\n",
      "ROWS: 177\n",
      "COLS: 51\n",
      "=========================\n",
      "\n",
      "[OK] wrote column list: ./outputs_appalachia_202207/noaa_columns_list.txt\n",
      "---- ALL COLUMNS ----\n",
      "  1. BEGIN_YEARMONTH\n",
      "  2. BEGIN_DAY\n",
      "  3. BEGIN_TIME\n",
      "  4. END_YEARMONTH\n",
      "  5. END_DAY\n",
      "  6. END_TIME\n",
      "  7. EPISODE_ID\n",
      "  8. EVENT_ID\n",
      "  9. STATE\n",
      " 10. STATE_FIPS\n",
      " 11. YEAR\n",
      " 12. MONTH_NAME\n",
      " 13. EVENT_TYPE\n",
      " 14. CZ_TYPE\n",
      " 15. CZ_FIPS\n",
      " 16. CZ_NAME\n",
      " 17. WFO\n",
      " 18. BEGIN_DATE_TIME\n",
      " 19. CZ_TIMEZONE\n",
      " 20. END_DATE_TIME\n",
      " 21. INJURIES_DIRECT\n",
      " 22. INJURIES_INDIRECT\n",
      " 23. DEATHS_DIRECT\n",
      " 24. DEATHS_INDIRECT\n",
      " 25. DAMAGE_PROPERTY\n",
      " 26. DAMAGE_CROPS\n",
      " 27. SOURCE\n",
      " 28. MAGNITUDE\n",
      " 29. MAGNITUDE_TYPE\n",
      " 30. FLOOD_CAUSE\n",
      " 31. CATEGORY\n",
      " 32. TOR_F_SCALE\n",
      " 33. TOR_LENGTH\n",
      " 34. TOR_WIDTH\n",
      " 35. TOR_OTHER_WFO\n",
      " 36. TOR_OTHER_CZ_STATE\n",
      " 37. TOR_OTHER_CZ_FIPS\n",
      " 38. TOR_OTHER_CZ_NAME\n",
      " 39. BEGIN_RANGE\n",
      " 40. BEGIN_AZIMUTH\n",
      " 41. BEGIN_LOCATION\n",
      " 42. END_RANGE\n",
      " 43. END_AZIMUTH\n",
      " 44. END_LOCATION\n",
      " 45. BEGIN_LAT\n",
      " 46. BEGIN_LON\n",
      " 47. END_LAT\n",
      " 48. END_LON\n",
      " 49. EPISODE_NARRATIVE\n",
      " 50. EVENT_NARRATIVE\n",
      " 51. DATA_SOURCE\n",
      "---------------------\n",
      "\n",
      "==== DETECTED IMPACT-RELATED FIELDS (pattern-based) ====\n",
      "[injuries] INJURIES_DIRECT | dtype=int64 | nonnull=177\n",
      "   examples: 0 | 0 | 0 | 0 | 0 | 0\n",
      "[injuries] INJURIES_INDIRECT | dtype=int64 | nonnull=177\n",
      "   examples: 0 | 0 | 0 | 0 | 0 | 0\n",
      "[deaths] DEATHS_DIRECT | dtype=int64 | nonnull=177\n",
      "   examples: 0 | 0 | 0 | 0 | 0 | 0\n",
      "[deaths] DEATHS_INDIRECT | dtype=int64 | nonnull=177\n",
      "   examples: 0 | 0 | 0 | 0 | 0 | 0\n",
      "[property_damage] DAMAGE_PROPERTY | dtype=object | nonnull=177\n",
      "   examples: 1.00K | 1.00K | 2.00K | 1.00K | 1.00K | 1.00K\n",
      "[crop_damage] DAMAGE_CROPS | dtype=object | nonnull=177\n",
      "   examples: 0.00K | 0.00K | 0.00K | 0.00K | 0.00K | 0.00K\n",
      "[narrative_text] EPISODE_NARRATIVE | dtype=object | nonnull=177\n",
      "   examples: A big push of monsoon moisture immediately followed the excessive heat event, and stormy conditions persisted into August. Many storms produced flash flooding, and some produced severe weather. | A big push of monsoon moisture immediately followed the excessive heat event, and stormy conditions persisted into August. Many storms produced flash flooding, and some produced severe weather. | A big push of monsoon moisture immediately followed the excessive heat event, and stormy conditions persisted into August. Many storms produced flash flooding, and some produced severe weather. | A big push of monsoon moisture immediately followed the excessive heat event, and stormy conditions persisted into August. Many storms produced flash flooding, and some produced severe weather. | A big push of monsoon moisture immediately followed the excessive heat event, and stormy conditions persisted into August. Many storms produced flash flooding, and some produced severe weather. | The stormy period which began on the 24th continued to bring severe weather and flash flooding through the end of the month.\n",
      "[narrative_text] EVENT_NARRATIVE | dtype=object | nonnull=177\n",
      "   examples: Sycamore Camp Rd was closed from Burro Creek Crossing to mile post 9. | Oatman-Topock Hwy was closed just north of I-40. | Pierce Ferry Rd was closed due to flooding of the Archibald Wash. | Flooding of Short Creek closed several streets in Colorado City. | White Hills Rd was closed. | I-15 southbound was flooded near Littlefield.\n",
      "[location_text] CZ_NAME | dtype=object | nonnull=177\n",
      "   examples: MOHAVE | MOHAVE | MOHAVE | MOHAVE | MOHAVE | MOHAVE\n",
      "[location_text] TOR_OTHER_CZ_NAME | dtype=float64 | nonnull=0\n",
      "[location_text] BEGIN_LOCATION | dtype=object | nonnull=177\n",
      "   examples: WIKIEUP | GOLDEN SHORES | DOLAN SPGS | COLORADO CITY | WILLOW BEACH | LITTLEFIELD\n",
      "[location_text] END_LOCATION | dtype=object | nonnull=177\n",
      "   examples: WIKIEUP | GOLDEN SHORES | DOLAN SPGS | COLORADO CITY | WILLOW BEACH | LITTLEFIELD\n",
      "[magnitude_severity] CZ_TYPE | dtype=object | nonnull=177\n",
      "   examples: C | C | C | C | C | C\n",
      "[magnitude_severity] DAMAGE_PROPERTY | dtype=object | nonnull=177\n",
      "   examples: 1.00K | 1.00K | 2.00K | 1.00K | 1.00K | 1.00K\n",
      "[magnitude_severity] DAMAGE_CROPS | dtype=object | nonnull=177\n",
      "   examples: 0.00K | 0.00K | 0.00K | 0.00K | 0.00K | 0.00K\n",
      "[magnitude_severity] SOURCE | dtype=object | nonnull=177\n",
      "   examples: County Official | County Official | County Official | Emergency Manager | County Official | Department of Highways\n",
      "[magnitude_severity] MAGNITUDE | dtype=float64 | nonnull=0\n",
      "[magnitude_severity] MAGNITUDE_TYPE | dtype=float64 | nonnull=0\n",
      "[magnitude_severity] FLOOD_CAUSE | dtype=object | nonnull=162\n",
      "   examples: Heavy Rain | Heavy Rain | Heavy Rain | Heavy Rain | Heavy Rain | Heavy Rain\n",
      "[magnitude_severity] TOR_F_SCALE | dtype=float64 | nonnull=0\n",
      "[magnitude_severity] DATA_SOURCE | dtype=object | nonnull=177\n",
      "   examples: CSV | CSV | CSV | CSV | CSV | CSV\n",
      "=======================================================\n",
      "\n",
      "==== DAMAGE-LIKE COLUMNS (content-based scan) ====\n",
      "[damage_like_scan] BEGIN_YEARMONTH | dtype=int64 | nonnull=177\n",
      "   examples: 202207 | 202207 | 202207 | 202207 | 202207 | 202207\n",
      "[damage_like_scan] BEGIN_DAY | dtype=int64 | nonnull=177\n",
      "   examples: 26 | 26 | 27 | 27 | 27 | 28\n",
      "[damage_like_scan] BEGIN_TIME | dtype=int64 | nonnull=177\n",
      "   examples: 1400 | 1905 | 1415 | 1500 | 1505 | 1935\n",
      "[damage_like_scan] END_YEARMONTH | dtype=int64 | nonnull=177\n",
      "   examples: 202207 | 202207 | 202207 | 202207 | 202207 | 202207\n",
      "[damage_like_scan] END_DAY | dtype=int64 | nonnull=177\n",
      "   examples: 26 | 26 | 27 | 27 | 27 | 28\n",
      "[damage_like_scan] END_TIME | dtype=int64 | nonnull=177\n",
      "   examples: 1600 | 2100 | 1630 | 1600 | 1700 | 2100\n",
      "[damage_like_scan] EPISODE_ID | dtype=int64 | nonnull=177\n",
      "   examples: 172910 | 172910 | 172910 | 172910 | 172910 | 172913\n",
      "[damage_like_scan] EVENT_ID | dtype=int64 | nonnull=177\n",
      "   examples: 1050202 | 1050205 | 1050206 | 1050207 | 1050208 | 1050219\n",
      "[damage_like_scan] STATE_FIPS | dtype=int64 | nonnull=177\n",
      "   examples: 4 | 4 | 4 | 4 | 4 | 4\n",
      "[damage_like_scan] YEAR | dtype=int64 | nonnull=177\n",
      "   examples: 2022 | 2022 | 2022 | 2022 | 2022 | 2022\n",
      "[damage_like_scan] CZ_FIPS | dtype=int64 | nonnull=177\n",
      "   examples: 15 | 15 | 15 | 15 | 15 | 15\n",
      "[damage_like_scan] INJURIES_DIRECT | dtype=int64 | nonnull=177\n",
      "   examples: 0 | 0 | 0 | 0 | 0 | 0\n",
      "[damage_like_scan] INJURIES_INDIRECT | dtype=int64 | nonnull=177\n",
      "   examples: 0 | 0 | 0 | 0 | 0 | 0\n",
      "[damage_like_scan] DEATHS_DIRECT | dtype=int64 | nonnull=177\n",
      "   examples: 0 | 0 | 0 | 0 | 0 | 0\n",
      "[damage_like_scan] DEATHS_INDIRECT | dtype=int64 | nonnull=177\n",
      "   examples: 0 | 0 | 0 | 0 | 0 | 0\n",
      "[damage_like_scan] DAMAGE_PROPERTY | dtype=object | nonnull=177\n",
      "   examples: 1.00K | 1.00K | 2.00K | 1.00K | 1.00K | 1.00K\n",
      "[damage_like_scan] DAMAGE_CROPS | dtype=object | nonnull=177\n",
      "   examples: 0.00K | 0.00K | 0.00K | 0.00K | 0.00K | 0.00K\n",
      "[damage_like_scan] BEGIN_RANGE | dtype=float64 | nonnull=177\n",
      "   examples: 13.0 | 2.0 | 5.0 | 1.0 | 16.0 | 1.0\n",
      "[damage_like_scan] END_RANGE | dtype=float64 | nonnull=177\n",
      "   examples: 12.0 | 2.0 | 5.0 | 1.0 | 16.0 | 1.0\n",
      "[damage_like_scan] BEGIN_LAT | dtype=float64 | nonnull=177\n",
      "   examples: 34.68 | 34.756 | 35.6954 | 36.9856 | 35.7086 | 36.895\n",
      "[damage_like_scan] END_LAT | dtype=float64 | nonnull=177\n",
      "   examples: 34.6806 | 34.7557 | 35.6948 | 36.9869 | 35.711 | 36.8934\n",
      "=================================================\n",
      "\n",
      "[OK] wrote detected-field summary: ./outputs_appalachia_202207/noaa_detected_impact_fields.csv\n",
      "\n",
      "==== BEST-GUESS FIELD MAPPING (for next plots) ====\n",
      "injuries        : INJURIES_DIRECT\n",
      "deaths          : DEATHS_DIRECT\n",
      "property_damage : DAMAGE_PROPERTY\n",
      "crop_damage     : DAMAGE_CROPS\n",
      "event_narrative : EPISODE_NARRATIVE\n",
      "location_text   : CZ_NAME\n",
      "==================================================\n",
      "\n",
      "[DONE] Now paste the 'BEST-GUESS FIELD MAPPING' output here, and Iâ€™ll generate the impact plots script.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "CSV_PATH = \"./storm_events_download_study/filtered/noaa_stormevents_2022_20220726_20220728_details.csv\"\n",
    "OUT_DIR = \"./outputs_appalachia_202207\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "OUT_COLS_TXT = os.path.join(OUT_DIR, \"noaa_columns_list.txt\")\n",
    "OUT_DETECTED = os.path.join(OUT_DIR, \"noaa_detected_impact_fields.csv\")\n",
    "\n",
    "\n",
    "def norm(s: str) -> str:\n",
    "    return re.sub(r\"[^a-z0-9]+\", \"_\", str(s).strip().lower()).strip(\"_\")\n",
    "\n",
    "def find_candidates(columns, patterns):\n",
    "    \"\"\"Return list of columns whose normalized name matches ANY regex pattern.\"\"\"\n",
    "    out = []\n",
    "    for c in columns:\n",
    "        nc = norm(c)\n",
    "        for pat in patterns:\n",
    "            if re.search(pat, nc):\n",
    "                out.append(c)\n",
    "                break\n",
    "    return out\n",
    "\n",
    "def preview_col(df, col, n=5):\n",
    "    s = df[col]\n",
    "    nonnull = s.dropna()\n",
    "    if len(nonnull) == 0:\n",
    "        return {\"col\": col, \"dtype\": str(s.dtype), \"nonnull\": 0, \"examples\": \"\"}\n",
    "    ex = nonnull.astype(str).head(n).tolist()\n",
    "    return {\"col\": col, \"dtype\": str(s.dtype), \"nonnull\": int(nonnull.shape[0]), \"examples\": \" | \".join(ex)}\n",
    "\n",
    "def is_damage_like(series: pd.Series) -> bool:\n",
    "    \"\"\"\n",
    "    Detect NOAA-style damage strings: 0, 10K, 2.5M, 1B, etc.\n",
    "    Works even if numeric.\n",
    "    \"\"\"\n",
    "    s = series.dropna()\n",
    "    if len(s) == 0:\n",
    "        return False\n",
    "    samp = s.astype(str).head(200).str.upper().str.strip()\n",
    "    # digits with optional decimal and suffix K/M/B\n",
    "    pat = re.compile(r\"^\\$?\\s*\\d+(\\.\\d+)?\\s*[KMB]?$\")\n",
    "    hits = sum(bool(pat.match(x)) for x in samp)\n",
    "    return hits >= max(3, int(0.05 * len(samp)))  # at least a few hits\n",
    "\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(CSV_PATH):\n",
    "        raise FileNotFoundError(f\"CSV not found: {CSV_PATH}\")\n",
    "\n",
    "    df = pd.read_csv(CSV_PATH, low_memory=False)\n",
    "    cols = list(df.columns)\n",
    "\n",
    "    print(\"\\n=========================\")\n",
    "    print(\"FILE:\", CSV_PATH)\n",
    "    print(\"ROWS:\", len(df))\n",
    "    print(\"COLS:\", len(cols))\n",
    "    print(\"=========================\\n\")\n",
    "\n",
    "    with open(OUT_COLS_TXT, \"w\", encoding=\"utf-8\") as f:\n",
    "        for c in cols:\n",
    "            f.write(c + \"\\n\")\n",
    "    print(\"[OK] wrote column list:\", OUT_COLS_TXT)\n",
    "\n",
    "    print(\"---- ALL COLUMNS ----\")\n",
    "    for i, c in enumerate(cols, 1):\n",
    "        print(f\"{i:3d}. {c}\")\n",
    "    print(\"---------------------\\n\")\n",
    "\n",
    "    patterns = {\n",
    "        \"injuries\": [\n",
    "            r\"injur\", r\"injuries\", r\"injuries_direct\", r\"injuries_indirect\",\n",
    "            r\"direct_inj\", r\"indirect_inj\"\n",
    "        ],\n",
    "        \"deaths\": [\n",
    "            r\"death\", r\"deaths\", r\"deaths_direct\", r\"deaths_indirect\",\n",
    "            r\"direct_death\", r\"indirect_death\", r\"fatal\"\n",
    "        ],\n",
    "        \"property_damage\": [\n",
    "            r\"damage_property\", r\"property_damage\", r\"dmg_property\", r\"prop_dmg\", r\"damage_prop\"\n",
    "        ],\n",
    "        \"crop_damage\": [\n",
    "            r\"damage_crops\", r\"crop_damage\", r\"dmg_crops\", r\"crop_dmg\"\n",
    "        ],\n",
    "        \"narrative_text\": [\n",
    "            r\"narrative\", r\"episode_narrative\", r\"event_narrative\", r\"comments\", r\"remark\"\n",
    "        ],\n",
    "        \"location_text\": [\n",
    "            r\"begin_location\", r\"end_location\", r\"location\", r\"cz_name\", r\"county\", r\"city\", r\"locality\"\n",
    "        ],\n",
    "        \"magnitude_severity\": [\n",
    "            r\"magnitude\", r\"mag\", r\"f_scale\", r\"tor_f_scale\", r\"fujita\", r\"cz_type\",\n",
    "            r\"flood_cause\", r\"severity\", r\"source\"\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    detected = []\n",
    "    for group, pats in patterns.items():\n",
    "        cands = find_candidates(cols, pats)\n",
    "        if cands:\n",
    "            for c in cands:\n",
    "                detected.append((group, c))\n",
    "\n",
    "    damage_like_cols = []\n",
    "    for c in cols:\n",
    "        try:\n",
    "            if is_damage_like(df[c]):\n",
    "                damage_like_cols.append(c)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    rows = []\n",
    "    print(\"==== DETECTED IMPACT-RELATED FIELDS (pattern-based) ====\")\n",
    "    if not detected:\n",
    "        print(\"None detected by name patterns.\")\n",
    "    else:\n",
    "        for group, c in detected:\n",
    "            info = preview_col(df, c, n=6)\n",
    "            rows.append({\"group\": group, **info})\n",
    "            print(f\"[{group}] {c} | dtype={info['dtype']} | nonnull={info['nonnull']}\")\n",
    "            if info[\"examples\"]:\n",
    "                print(\"   examples:\", info[\"examples\"])\n",
    "    print(\"=======================================================\\n\")\n",
    "\n",
    "    print(\"==== DAMAGE-LIKE COLUMNS ====\")\n",
    "    if not damage_like_cols:\n",
    "        print(\"None detected by damage-string scan.\")\n",
    "    else:\n",
    "        for c in damage_like_cols:\n",
    "            info = preview_col(df, c, n=6)\n",
    "            rows.append({\"group\": \"damage_like_scan\", **info})\n",
    "            print(f\"[damage_like_scan] {c} | dtype={info['dtype']} | nonnull={info['nonnull']}\")\n",
    "            if info[\"examples\"]:\n",
    "                print(\"   examples:\", info[\"examples\"])\n",
    "    print(\"==========================\\n\")\n",
    "\n",
    "    if rows:\n",
    "        out = pd.DataFrame(rows).drop_duplicates(subset=[\"group\", \"col\"])\n",
    "        out.to_csv(OUT_DETECTED, index=False)\n",
    "        print(\"[OK] wrote detected-field summary:\", OUT_DETECTED)\n",
    "    else:\n",
    "        print(\"[WARN] No detected fields to write.\")\n",
    "\n",
    "    def best_guess(group_names):\n",
    "        cands = [r[\"col\"] for r in rows if r[\"group\"] in group_names]\n",
    "        # Prefer columns with most non-null values\n",
    "        best = None\n",
    "        best_n = -1\n",
    "        for c in cands:\n",
    "            nn = int(df[c].notna().sum())\n",
    "            if nn > best_n:\n",
    "                best_n = nn\n",
    "                best = c\n",
    "        return best\n",
    "\n",
    "    guess = {\n",
    "        \"injuries\": best_guess([\"injuries\"]),\n",
    "        \"deaths\": best_guess([\"deaths\"]),\n",
    "        \"property_damage\": best_guess([\"property_damage\", \"damage_like_scan\"]),\n",
    "        \"crop_damage\": best_guess([\"crop_damage\", \"damage_like_scan\"]),\n",
    "        \"event_narrative\": best_guess([\"narrative_text\"]),\n",
    "        \"location_text\": best_guess([\"location_text\"]),\n",
    "    }\n",
    "\n",
    "    for k, v in guess.items():\n",
    "        print(f\"{k:16s}: {v}\")\n",
    "    print(\"==================================================\\n\")\n",
    "\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f29b4a77-ac02-4094-a71d-10b3c8b6c535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded: ./storm_events_download_study/filtered/noaa_stormevents_2022_20220726_20220728_details.csv rows= 177\n",
      "[INFO] After filters rows: 64\n",
      "[INFO] Event types: {'Flash Flood': 53, 'Flood': 11}\n",
      "[OK] Saved: ./outputs_appalachia_202207_impacts/impact_summary_totals.txt\n",
      "[OK] Saved: ./outputs_appalachia_202207_impacts/bar_damage_by_state.png\n",
      "[OK] Saved: ./outputs_appalachia_202207_impacts/bar_damage_by_top_counties.png\n",
      "[OK] Saved: ./outputs_appalachia_202207_impacts/map_points_damage_bubbles.png\n",
      "[OK] Saved: ./outputs_appalachia_202207_impacts/map_state_property_damage.png\n",
      "[OK] Saved: ./outputs_appalachia_202207_impacts/bar_flood_cause_counts.png\n",
      "[OK] Saved: ./outputs_appalachia_202207_impacts/bar_source_counts.png\n",
      "[OK] Saved: ./outputs_appalachia_202207_impacts/bar_top_keywords_narratives.png\n",
      "[OK] Saved: ./outputs_appalachia_202207_impacts/pareto_damage_by_county.png\n",
      "[DONE] Impact outputs written to: ./outputs_appalachia_202207_impacts\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "EMDS 501 / Natural & Technological Disaster Risks\n",
    "\"Other side\" of NOAA Storm Events for the study period:\n",
    "IMPACTS + SEVERITY + HUMAN/Economic dimensions (no interpolation).\n",
    "\n",
    "Inputs:\n",
    "  - Your merged NOAA details+locations CSV (study period)\n",
    "\n",
    "Outputs (in OUT_DIR):\n",
    "  1) impact_summary_totals.txt\n",
    "  2) bar_damage_by_state.png                 (total $ property damage by state)\n",
    "  3) bar_damage_by_top_counties.png          (top counties by total $ property damage)\n",
    "  4) map_points_damage_bubbles.png           (points map sized by property damage, colored by event type)\n",
    "  5) map_state_property_damage.png           (state choropleth by total $ property damage + table below)\n",
    "  6) bar_flood_cause_counts.png              (counts by FLOOD_CAUSE)\n",
    "  7) bar_source_counts.png                   (counts by SOURCE)\n",
    "  8) bar_top_keywords_narratives.png         (top words from narratives, simple frequency)\n",
    "  9) pareto_damage_by_county.png             (damage concentration / Pareto)\n",
    "\n",
    "Notes:\n",
    "- Parses DAMAGE_PROPERTY / DAMAGE_CROPS strings like \"0.20K\", \"500.00K\", \"1.2M\", \"0\".\n",
    "- If injuries/deaths are all zeros (common), it still prints totals (useful for your writeup).\n",
    "- Spatial plots use a 2-row layout (map on top, table below) so tables never cover the map.\n",
    "- If the Natural Earth states shapefile is missing, it auto-downloads + unzips it.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "import urllib.request\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "\n",
    "EVENTS_CSV = \"./storm_events_download_study/filtered/noaa_stormevents_2022_20220726_20220728_details.csv\"\n",
    "\n",
    "STATES_SHP = \"./ne_50m_admin_1_states_provinces_lakes/ne_50m_admin_1_states_provinces_lakes.shp\"\n",
    "\n",
    "FOCUS_STATES = {\"KENTUCKY\", \"WEST VIRGINIA\", \"VIRGINIA\", \"TENNESSEE\", \"OHIO\"}\n",
    "\n",
    "BBOX = dict(lon_min=-91.0, lon_max=-79.0, lat_min=34.0, lat_max=41.5)\n",
    "\n",
    "# Flood events only \n",
    "KEEP_EVENT_TYPES = {\"FLASH FLOOD\", \"FLOOD\"}\n",
    "\n",
    "OUT_DIR = \"./outputs_appalachia_202207_impacts\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "OUT_TXT_SUMMARY     = os.path.join(OUT_DIR, \"impact_summary_totals.txt\")\n",
    "OUT_BAR_STATE_DMG   = os.path.join(OUT_DIR, \"bar_damage_by_state.png\")\n",
    "OUT_BAR_COUNTY_DMG  = os.path.join(OUT_DIR, \"bar_damage_by_top_counties.png\")\n",
    "OUT_MAP_BUBBLES_DMG = os.path.join(OUT_DIR, \"map_points_damage_bubbles.png\")\n",
    "OUT_MAP_STATE_DMG   = os.path.join(OUT_DIR, \"map_state_property_damage.png\")\n",
    "OUT_BAR_CAUSE       = os.path.join(OUT_DIR, \"bar_flood_cause_counts.png\")\n",
    "OUT_BAR_SOURCE      = os.path.join(OUT_DIR, \"bar_source_counts.png\")\n",
    "OUT_BAR_WORDS       = os.path.join(OUT_DIR, \"bar_top_keywords_narratives.png\")\n",
    "OUT_PARETO_COUNTY   = os.path.join(OUT_DIR, \"pareto_damage_by_county.png\")\n",
    "\n",
    "TOP_N_COUNTIES = 10\n",
    "TOP_N_WORDS    = 15\n",
    "\n",
    "NE_URL = \"https://naturalearth.s3.amazonaws.com/50m_cultural/ne_50m_admin_1_states_provinces.zip\"\n",
    "NE_DIR = \"./ne_50m_admin_1_states_provinces\"\n",
    "NE_ZIP = os.path.join(NE_DIR, \"ne_50m_admin_1_states_provinces.zip\")\n",
    "NE_SHP = os.path.join(NE_DIR, \"ne_50m_admin_1_states_provinces.shp\")\n",
    "\n",
    "\n",
    "def ensure_naturalearth_states():\n",
    "\n",
    "    os.makedirs(NE_DIR, exist_ok=True)\n",
    "    if os.path.exists(NE_SHP):\n",
    "        return NE_SHP\n",
    "\n",
    "    print(\"[INFO] Downloading:\", NE_URL)\n",
    "    urllib.request.urlretrieve(NE_URL, NE_ZIP)\n",
    "\n",
    "    print(\"[INFO] Unzipping:\", NE_ZIP)\n",
    "    with zipfile.ZipFile(NE_ZIP, \"r\") as z:\n",
    "        z.extractall(NE_DIR)\n",
    "\n",
    "    if not os.path.exists(NE_SHP):\n",
    "        raise FileNotFoundError(\"Natural Earth unzip succeeded but .shp not found: \" + NE_SHP)\n",
    "\n",
    "    return NE_SHP\n",
    "\n",
    "\n",
    "def find_first_col(df, candidates):\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    for cand in candidates:\n",
    "        k = cand.lower()\n",
    "        if k in cols:\n",
    "            return cols[k]\n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_noaa_dt(series):\n",
    "    s = series.astype(str).str.strip()\n",
    "    s = s.replace({\"\": pd.NA, \"nan\": pd.NA, \"NaT\": pd.NA})\n",
    "\n",
    "    dt = pd.to_datetime(s, errors=\"coerce\", format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    if dt.notna().any():\n",
    "        return dt\n",
    "    dt2 = pd.to_datetime(s, errors=\"coerce\", format=\"%d-%b-%y %H:%M:%S\")\n",
    "    if dt2.notna().any():\n",
    "        return dt2\n",
    "    dt3 = pd.to_datetime(s, errors=\"coerce\", format=\"%d-%b-%y %I:%M:%S %p\")\n",
    "    if dt3.notna().any():\n",
    "        return dt3\n",
    "\n",
    "    return pd.to_datetime(s, errors=\"coerce\")\n",
    "\n",
    "\n",
    "def is_us_states_layer(gdf_states):\n",
    "    if \"admin\" in gdf_states.columns:\n",
    "        return gdf_states[gdf_states[\"admin\"].astype(str) == \"United States of America\"].copy()\n",
    "    return gdf_states.copy()\n",
    "\n",
    "\n",
    "def normalize_state_name(s):\n",
    "    return str(s).strip().upper()\n",
    "\n",
    "\n",
    "def safe_title(s):\n",
    "    return str(s).strip().title()\n",
    "\n",
    "\n",
    "def damage_to_usd(x):\n",
    "    \"\"\"\n",
    "    NOAA Storm Events damage format examples:\n",
    "      \"0\", \"0.20K\", \"500.00K\", \"1.2M\", \"2B\"\n",
    "    Returns float USD.\n",
    "    \"\"\"\n",
    "    if pd.isna(x):\n",
    "        return 0.0\n",
    "    s = str(x).strip().upper().replace(\",\", \"\")\n",
    "    if s in (\"\", \"NAN\", \"NONE\"):\n",
    "        return 0.0\n",
    "\n",
    "    s = s.lstrip(\"$\").strip()\n",
    "\n",
    "    # pure numeric?\n",
    "    try:\n",
    "        return float(s)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    m = re.match(r\"^(\\d+(\\.\\d+)?)\\s*([KMB])?$\", s)\n",
    "    if not m:\n",
    "        s2 = re.sub(r\"\\s+\", \"\", s)\n",
    "        m2 = re.match(r\"^(\\d+(\\.\\d+)?)\\s*([KMB])?$\", s2)\n",
    "        if not m2:\n",
    "            return 0.0\n",
    "        m = m2\n",
    "\n",
    "    val = float(m.group(1))\n",
    "    suf = m.group(3)\n",
    "    mult = 1.0\n",
    "    if suf == \"K\":\n",
    "        mult = 1e3\n",
    "    elif suf == \"M\":\n",
    "        mult = 1e6\n",
    "    elif suf == \"B\":\n",
    "        mult = 1e9\n",
    "    return val * mult\n",
    "\n",
    "\n",
    "def fmt_money(x):\n",
    "    x = float(x)\n",
    "    if x >= 1e9:\n",
    "        return f\"${x/1e9:.2f}B\"\n",
    "    if x >= 1e6:\n",
    "        return f\"${x/1e6:.2f}M\"\n",
    "    if x >= 1e3:\n",
    "        return f\"${x/1e3:.2f}K\"\n",
    "    return f\"${x:.0f}\"\n",
    "\n",
    "\n",
    "def make_table(ax, header, rows, fontsize=10):\n",
    "    data = [header] + rows\n",
    "    t = ax.table(cellText=data, cellLoc=\"center\", loc=\"center\")\n",
    "    t.auto_set_font_size(False)\n",
    "    t.set_fontsize(fontsize)\n",
    "    for j in range(len(header)):\n",
    "        t[(0, j)].set_text_props(fontweight=\"bold\")\n",
    "    ax.axis(\"off\")\n",
    "    return t\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    # simple tokenization for narrative keywords\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    toks = [w for w in text.split() if len(w) >= 4]\n",
    "    stop = {\n",
    "        \"with\",\"that\",\"this\",\"from\",\"were\",\"over\",\"into\",\"also\",\"fell\",\"kept\",\"near\",\"during\",\n",
    "        \"resulted\",\"county\",\"counties\",\"water\",\"rain\",\"flood\",\"flooding\",\"flash\",\"roads\",\n",
    "        \"roadway\",\"local\",\"creeks\",\"streams\",\"banks\",\"community\",\"early\",\"morning\",\"hours\",\n",
    "        \"evening\",\"night\",\"area\",\"state\",\"along\",\"continued\",\"moved\",\"across\",\"remained\",\n",
    "        \"period\",\"spots\",\"inches\",\"several\",\"including\",\"entered\",\"first\",\"floor\",\"building\"\n",
    "    }\n",
    "    return [w for w in toks if w not in stop]\n",
    "\n",
    "\n",
    "def bar_with_labels(ax, labels, values, label_fmt=None, rotation=25):\n",
    "    x = np.arange(len(labels))\n",
    "    ax.bar(x, values)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels, rotation=rotation, ha=\"right\")\n",
    "    for i, v in enumerate(values):\n",
    "        txt = label_fmt(v) if label_fmt else str(v)\n",
    "        ax.text(i, v, txt, ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(EVENTS_CSV):\n",
    "        raise FileNotFoundError(\"Missing EVENTS_CSV: \" + EVENTS_CSV)\n",
    "\n",
    "    states_shp_path = STATES_SHP\n",
    "    if not os.path.exists(states_shp_path):\n",
    "        states_shp_path = ensure_naturalearth_states()\n",
    "\n",
    "    df = pd.read_csv(EVENTS_CSV, low_memory=False)\n",
    "    print(\"[INFO] Loaded:\", EVENTS_CSV, \"rows=\", len(df))\n",
    "\n",
    "    lat_col = find_first_col(df, [\"LATITUDE\", \"BEGIN_LAT\", \"LAT\", \"BEGIN_LATITUDE\"])\n",
    "    lon_col = find_first_col(df, [\"LONGITUDE\", \"BEGIN_LON\", \"LON\", \"BEGIN_LONGITUDE\"])\n",
    "    time_col = find_first_col(df, [\"BEGIN_DATE_TIME\"])\n",
    "    etype_col = find_first_col(df, [\"EVENT_TYPE\"])\n",
    "    state_col = find_first_col(df, [\"STATE\"])\n",
    "    county_col = find_first_col(df, [\"CZ_NAME\", \"COUNTY\"])\n",
    "\n",
    "    dmgp_col = find_first_col(df, [\"DAMAGE_PROPERTY\"])\n",
    "    dmgc_col = find_first_col(df, [\"DAMAGE_CROPS\"])\n",
    "    injd_col = find_first_col(df, [\"INJURIES_DIRECT\"])\n",
    "    inji_col = find_first_col(df, [\"INJURIES_INDIRECT\"])\n",
    "    dead_col = find_first_col(df, [\"DEATHS_DIRECT\"])\n",
    "    deai_col = find_first_col(df, [\"DEATHS_INDIRECT\"])\n",
    "    cause_col = find_first_col(df, [\"FLOOD_CAUSE\"])\n",
    "    src_col   = find_first_col(df, [\"SOURCE\"])\n",
    "    epn_col   = find_first_col(df, [\"EPISODE_NARRATIVE\"])\n",
    "    evn_col   = find_first_col(df, [\"EVENT_NARRATIVE\"])\n",
    "\n",
    "    for must in [lat_col, lon_col, etype_col, state_col, dmgp_col]:\n",
    "        if must is None:\n",
    "            raise ValueError(\"Missing a required column. Check the CSV schema.\")\n",
    "\n",
    "    dff = df.copy()\n",
    "    dff[\"dt\"] = parse_noaa_dt(dff[time_col]) if time_col else pd.NaT\n",
    "    dff[\"state_u\"] = dff[state_col].map(normalize_state_name)\n",
    "    dff[\"state_t\"] = dff[state_col].map(safe_title)\n",
    "    dff[\"etype_u\"] = dff[etype_col].astype(str).str.strip().str.upper()\n",
    "    dff[\"etype_t\"] = dff[etype_col].astype(str).str.strip()\n",
    "    dff[\"county\"]  = dff[county_col].astype(str).str.strip().str.title() if county_col else \"\"\n",
    "\n",
    "    dff = dff[dff[\"state_u\"].isin(FOCUS_STATES)].copy()\n",
    "\n",
    "    dff = dff[\n",
    "        (pd.to_numeric(dff[lon_col], errors=\"coerce\") >= BBOX[\"lon_min\"]) &\n",
    "        (pd.to_numeric(dff[lon_col], errors=\"coerce\") <= BBOX[\"lon_max\"]) &\n",
    "        (pd.to_numeric(dff[lat_col], errors=\"coerce\") >= BBOX[\"lat_min\"]) &\n",
    "        (pd.to_numeric(dff[lat_col], errors=\"coerce\") <= BBOX[\"lat_max\"])\n",
    "    ].copy()\n",
    "\n",
    "    if KEEP_EVENT_TYPES is not None:\n",
    "        keep_u = {k.strip().upper() for k in KEEP_EVENT_TYPES}\n",
    "        dff = dff[dff[\"etype_u\"].isin(keep_u)].copy()\n",
    "\n",
    "    print(\"[INFO] After filters rows:\", len(dff))\n",
    "    print(\"[INFO] Event types:\", dff[\"etype_t\"].value_counts().to_dict())\n",
    "\n",
    "    # Damage numeric\n",
    "    dff[\"prop_usd\"] = dff[dmgp_col].map(damage_to_usd).astype(float)\n",
    "    dff[\"crop_usd\"] = dff[dmgc_col].map(damage_to_usd).astype(float) if dmgc_col else 0.0\n",
    "\n",
    "    # Injury/death totals\n",
    "    inj_direct = float(dff[injd_col].sum()) if injd_col else 0.0\n",
    "    inj_indir  = float(dff[inji_col].sum()) if inji_col else 0.0\n",
    "    dea_direct = float(dff[dead_col].sum()) if dead_col else 0.0\n",
    "    dea_indir  = float(dff[deai_col].sum()) if deai_col else 0.0\n",
    "\n",
    "    total_prop = float(dff[\"prop_usd\"].sum())\n",
    "    total_crop = float(dff[\"crop_usd\"].sum())\n",
    "\n",
    "    # Write summary\n",
    "    with open(OUT_TXT_SUMMARY, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"Study Period Impact Summary\\n\")\n",
    "        f.write(\"===========================\\n\")\n",
    "        f.write(f\"Rows (reports): {len(dff)}\\n\")\n",
    "        f.write(f\"Total property damage: {fmt_money(total_prop)}\\n\")\n",
    "        f.write(f\"Total crop damage:     {fmt_money(total_crop)}\\n\")\n",
    "        f.write(f\"Injuries (direct):     {inj_direct:.0f}\\n\")\n",
    "        f.write(f\"Injuries (indirect):   {inj_indir:.0f}\\n\")\n",
    "        f.write(f\"Deaths (direct):       {dea_direct:.0f}\\n\")\n",
    "        f.write(f\"Deaths (indirect):     {dea_indir:.0f}\\n\")\n",
    "    print(\"[OK] Saved:\", OUT_TXT_SUMMARY)\n",
    "\n",
    "    # Geo points\n",
    "    gdf_pts = gpd.GeoDataFrame(\n",
    "        dff,\n",
    "        geometry=gpd.points_from_xy(dff[lon_col], dff[lat_col]),\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "\n",
    "    # Load states outlines (focus)\n",
    "    gdf_states = is_us_states_layer(gpd.read_file(states_shp_path))\n",
    "\n",
    "    name_col = find_first_col(gdf_states, [\"name\", \"name_en\", \"name_long\", \"NAME\", \"NAME_EN\"])\n",
    "    if name_col is None:\n",
    "\n",
    "        non_geom_cols = [c for c in gdf_states.columns if c != \"geometry\"]\n",
    "        if not non_geom_cols:\n",
    "            raise ValueError(\"States shapefile has no usable name columns.\")\n",
    "        name_col = non_geom_cols[0]\n",
    "\n",
    "    gdf_states = gdf_states.copy()\n",
    "    gdf_states[\"STATE_T\"] = gdf_states[name_col].astype(str).str.strip().str.title()\n",
    "    gdf_states[\"STATE_U\"] = gdf_states[name_col].astype(str).str.strip().str.upper()\n",
    "\n",
    "    states_focus = gdf_states[gdf_states[\"STATE_U\"].isin(FOCUS_STATES)].copy()\n",
    "    if len(states_focus) == 0:\n",
    "        states_focus = gdf_states.copy()\n",
    "\n",
    "    dmg_by_state = dff.groupby(\"state_t\")[\"prop_usd\"].sum().sort_values(ascending=False)\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    bar_with_labels(ax, list(dmg_by_state.index), list(dmg_by_state.values), label_fmt=fmt_money, rotation=25)\n",
    "    ax.set_title(\"Total Property Damage by State \\nCentral Appalachia - July 26-28, 2022\", fontsize=14)\n",
    "    ax.set_ylabel(\"Property damage (USD)\")\n",
    "    ax.grid(True, axis=\"y\", linewidth=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT_BAR_STATE_DMG, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    print(\"[OK] Saved:\", OUT_BAR_STATE_DMG)\n",
    "\n",
    "    if county_col:\n",
    "        dmg_by_cty = dff.groupby(\"county\")[\"prop_usd\"].sum().sort_values(ascending=False).head(TOP_N_COUNTIES)\n",
    "        fig, ax = plt.subplots(figsize=(11, 5))\n",
    "        bar_with_labels(ax, list(dmg_by_cty.index), list(dmg_by_cty.values), label_fmt=fmt_money, rotation=30)\n",
    "        ax.set_title(\n",
    "            f\"Top {TOP_N_COUNTIES} Counties by Property Damage\\nCentral Appalachia - July 26-28, 2022\",\n",
    "            fontsize=14\n",
    "        )\n",
    "        ax.set_ylabel(\"Property damage (USD)\")\n",
    "        ax.grid(True, axis=\"y\", linewidth=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(OUT_BAR_COUNTY_DMG, dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "        print(\"[OK] Saved:\", OUT_BAR_COUNTY_DMG)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 9))\n",
    "    states_focus.to_crs(\"EPSG:4326\").boundary.plot(ax=ax, linewidth=1.4, edgecolor=\"0.25\")\n",
    "\n",
    "    vals = gdf_pts[\"prop_usd\"].values.astype(float)\n",
    "    if len(vals) > 0 and np.isfinite(vals).any():\n",
    "        p95 = np.nanpercentile(vals, 95)\n",
    "        p95 = max(float(p95), 1.0)\n",
    "        size = 30 + 220 * np.clip(vals / p95, 0, 1)  # 30..250\n",
    "    else:\n",
    "        size = np.full(len(gdf_pts), 30.0)\n",
    "\n",
    "    etype_colors = {\"FLASH FLOOD\": \"tab:blue\", \"FLOOD\": \"tab:orange\"}\n",
    "    colors = gdf_pts[\"etype_u\"].map(etype_colors).fillna(\"tab:gray\")\n",
    "\n",
    "    ax.scatter(\n",
    "        gdf_pts.geometry.x,\n",
    "        gdf_pts.geometry.y,\n",
    "        s=size,\n",
    "        c=colors,\n",
    "        alpha=0.75,\n",
    "        edgecolors=\"none\"\n",
    "    )\n",
    "\n",
    "    ax.set_xlim(BBOX[\"lon_min\"], BBOX[\"lon_max\"])\n",
    "    ax.set_ylim(BBOX[\"lat_min\"], BBOX[\"lat_max\"])\n",
    "    ax.set_title(\n",
    "        \"NOAA Flood Reports Sized by Property Damage (Study Period)\\nCentral Appalachia - July 26-28, 2022\",\n",
    "        fontsize=20\n",
    "    )\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"\")\n",
    "    ax.set_frame_on(False)\n",
    "\n",
    "    # Legend: event type\n",
    "    for k, c in etype_colors.items():\n",
    "        ax.scatter([], [], s=60, c=c, label=k.title(), alpha=0.9)\n",
    "\n",
    "    # Legend: bubble sizes (relative damage)\n",
    "    for frac, lab in [(0.25, \"Low\"), (0.60, \"Medium\"), (1.00, \"High\")]:\n",
    "        ax.scatter([], [], s=30 + 220 * frac, c=\"0.3\", label=f\"Damage: {lab}\", alpha=0.3)\n",
    "\n",
    "    ax.legend(loc=\"upper right\", frameon=True, title=\"Legend\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT_MAP_BUBBLES_DMG, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    print(\"[OK] Saved:\", OUT_MAP_BUBBLES_DMG)\n",
    "\n",
    "    dmg_state = dff.groupby(\"state_u\")[\"prop_usd\"].sum().sort_values(ascending=False)\n",
    "    df_merge = dmg_state.reset_index()\n",
    "    df_merge.columns = [\"STATE_U\", \"Prop_USD\"]\n",
    "\n",
    "    states_plot = states_focus.copy()\n",
    "    states_plot = states_plot.merge(df_merge, on=\"STATE_U\", how=\"left\")\n",
    "    states_plot[\"Prop_USD\"] = states_plot[\"Prop_USD\"].fillna(0.0)\n",
    "\n",
    "    # log scale for better visualization (keep labels as real dollars)\n",
    "    states_plot[\"Prop_USD_LOG10\"] = np.log10(states_plot[\"Prop_USD\"].clip(lower=1.0))\n",
    "\n",
    "    fig = plt.figure(figsize=(14, 11))\n",
    "    gs = GridSpec(nrows=2, ncols=1, height_ratios=[4.2, 1.4], hspace=0.05)\n",
    "\n",
    "    ax_map = fig.add_subplot(gs[0, 0])\n",
    "    ax_tbl = fig.add_subplot(gs[1, 0])\n",
    "\n",
    "    states_plot.plot(\n",
    "        column=\"Prop_USD_LOG10\",\n",
    "        cmap=\"YlOrRd\",\n",
    "        linewidth=1.0,\n",
    "        edgecolor=\"0.6\",\n",
    "        legend=True,\n",
    "        legend_kwds={\"label\": r\"$\\log_{10}(\\mathrm{Total\\ property\\ damage\\ [USD]})$\"},\n",
    "        ax=ax_map\n",
    "    )\n",
    "\n",
    "    for _, row in states_plot.iterrows():\n",
    "        if row[\"geometry\"].is_empty:\n",
    "            continue\n",
    "        pt = row[\"geometry\"].representative_point()\n",
    "        ax_map.text(\n",
    "            pt.x,\n",
    "            pt.y,\n",
    "            fmt_money(row[\"Prop_USD\"]),\n",
    "            ha=\"center\",\n",
    "            fontsize=12,\n",
    "            fontweight=\"bold\"\n",
    "        )\n",
    "\n",
    "    ax_map.set_title(\n",
    "        \"Property Damage by State\\nCentral Appalachia - July 26-28, 2022\",\n",
    "        fontsize=22\n",
    "    )\n",
    "    ax_map.set_xticks([])\n",
    "    ax_map.set_yticks([])\n",
    "    ax_map.set_xlabel(\"\")\n",
    "    ax_map.set_ylabel(\"\")\n",
    "    ax_map.set_frame_on(False)\n",
    "\n",
    "    top3 = states_plot.sort_values(\"Prop_USD\", ascending=False).head(3)\n",
    "    rows = [[r[\"STATE_T\"], fmt_money(r[\"Prop_USD\"])] for _, r in top3.iterrows()]\n",
    "    make_table(ax_tbl, [\"State\", \"Total Property Damage\"], rows, fontsize=12)\n",
    "\n",
    "    plt.savefig(OUT_MAP_STATE_DMG, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    print(\"[OK] Saved:\", OUT_MAP_STATE_DMG)\n",
    "\n",
    "    if cause_col:\n",
    "        cause = dff[cause_col].fillna(\"Unknown\").astype(str).str.strip()\n",
    "        cause_counts = cause.value_counts().head(12)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(11, 5))\n",
    "        bar_with_labels(ax, list(cause_counts.index), list(cause_counts.values), label_fmt=lambda v: str(int(v)), rotation=25)\n",
    "        ax.set_title(\"Flood Cause (Counts) - Study Period\\nCentral Appalachia - July 26-28, 2022\", fontsize=14)\n",
    "        ax.set_ylabel(\"Number of reports\")\n",
    "        ax.grid(True, axis=\"y\", linewidth=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(OUT_BAR_CAUSE, dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "        print(\"[OK] Saved:\", OUT_BAR_CAUSE)\n",
    "\n",
    "    if src_col:\n",
    "        src = dff[src_col].fillna(\"Unknown\").astype(str).str.strip()\n",
    "        src_counts = src.value_counts().head(12)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(11, 5))\n",
    "        bar_with_labels(ax, list(src_counts.index), list(src_counts.values), label_fmt=lambda v: str(int(v)), rotation=25)\n",
    "        ax.set_title(\"NOAA Storm Events Reporting Source (Counts)\\nCentral Appalachia - July 26-28, 2022\", fontsize=14)\n",
    "        ax.set_ylabel(\"Number of reports\")\n",
    "        ax.grid(True, axis=\"y\", linewidth=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(OUT_BAR_SOURCE, dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "        print(\"[OK] Saved:\", OUT_BAR_SOURCE)\n",
    "\n",
    "    texts = []\n",
    "    if epn_col:\n",
    "        texts.append(dff[epn_col].fillna(\"\").astype(str))\n",
    "    if evn_col:\n",
    "        texts.append(dff[evn_col].fillna(\"\").astype(str))\n",
    "\n",
    "    if texts:\n",
    "        all_text = \" \".join(pd.concat(texts, axis=0).tolist())\n",
    "        toks = tokenize(all_text)\n",
    "        if toks:\n",
    "            vc = pd.Series(toks).value_counts().head(TOP_N_WORDS)\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(11, 5))\n",
    "            bar_with_labels(ax, list(vc.index), list(vc.values), label_fmt=lambda v: str(int(v)), rotation=25)\n",
    "            ax.set_title(\"Top Narrative Keywords (Simple Frequency)\\nCentral Appalachia - July 26-28, 2022\", fontsize=14)\n",
    "            ax.set_ylabel(\"Count\")\n",
    "            ax.grid(True, axis=\"y\", linewidth=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(OUT_BAR_WORDS, dpi=300, bbox_inches=\"tight\")\n",
    "            plt.close(fig)\n",
    "            print(\"[OK] Saved:\", OUT_BAR_WORDS)\n",
    "        else:\n",
    "            print(\"[WARN] No tokens extracted from narratives (unexpected).\")\n",
    "\n",
    "    # county concentration of property damage\n",
    "    if county_col:\n",
    "        tmp = dff.groupby(\"county\")[\"prop_usd\"].sum().sort_values(ascending=False)\n",
    "        pareto = tmp.reset_index()\n",
    "        pareto.columns = [\"county\", \"prop_usd\"]\n",
    "        denom = max(float(tmp.sum()), 1.0)\n",
    "        pareto[\"cum_pct\"] = 100.0 * pareto[\"prop_usd\"].cumsum() / denom\n",
    "\n",
    "        top = pareto.head(15)\n",
    "\n",
    "        fig, ax1 = plt.subplots(figsize=(12, 5))\n",
    "        x = np.arange(len(top[\"county\"]))\n",
    "        ax1.bar(x, top[\"prop_usd\"])\n",
    "        ax1.set_ylabel(\"Property damage (USD)\")\n",
    "        ax1.set_xticks(x)\n",
    "        ax1.set_xticklabels(top[\"county\"], rotation=30, ha=\"right\")\n",
    "        ax1.grid(True, axis=\"y\", linewidth=0.3)\n",
    "\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(x, top[\"cum_pct\"], marker=\"o\")\n",
    "        ax2.set_ylabel(\"Cumulative % of total damage\")\n",
    "        ax2.set_ylim(0, 105)\n",
    "\n",
    "        ax1.set_title(\"County Concentration of Property Damage \\nCentral Appalachia - July 26-28, 2022\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(OUT_PARETO_COUNTY, dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "        print(\"[OK] Saved:\", OUT_PARETO_COUNTY)\n",
    "\n",
    "    print(\"[DONE] Impact outputs written to:\", OUT_DIR)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbaa9e5-7c24-472a-b1a8-5c4ccc26838c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36c3a7a7-c710-4cf0-87dc-089647982f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct deaths: 41\n",
      "Indirect deaths: 1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "EVENTS_CSV = \"./storm_events_download_study/filtered/noaa_stormevents_2022_20220726_20220728_details.csv\"\n",
    "\n",
    "df = pd.read_csv(EVENTS_CSV, low_memory=False)\n",
    "\n",
    "def find_first_col(df, candidates):\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    for cand in candidates:\n",
    "        if cand.lower() in cols:\n",
    "            return cols[cand.lower()]\n",
    "    return None\n",
    "\n",
    "dead_col = find_first_col(df, [\"DEATHS_DIRECT\"])\n",
    "deai_col = find_first_col(df, [\"DEATHS_INDIRECT\"])\n",
    "state_col = find_first_col(df, [\"STATE\"])\n",
    "\n",
    "dea_direct = float(df[dead_col].sum()) if dead_col else 0\n",
    "dea_indir  = float(df[deai_col].sum()) if deai_col else 0\n",
    "\n",
    "print(\"Direct deaths:\", int(dea_direct))\n",
    "print(\"Indirect deaths:\", int(dea_indir))\n",
    "labels = [\"Direct\", \"Indirect\"]\n",
    "vals = [dea_direct, dea_indir]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "x = np.arange(len(labels))\n",
    "\n",
    "ax.bar(x, vals)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "\n",
    "for i, v in enumerate(vals):\n",
    "    ax.text(i, v, str(int(v)), ha=\"center\", va=\"bottom\")\n",
    "\n",
    "ax.set_title(\"Fatalities (NOAA Storm Events)\\nJuly 26â€“28, 2022\")\n",
    "ax.set_ylabel(\"Number of deaths\")\n",
    "ax.grid(True, axis=\"y\", linewidth=0.3)\n",
    "\n",
    "plt.close()\n",
    "\n",
    "deaths_by_state = df.groupby(state_col)[dead_col].sum().sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "\n",
    "x = np.arange(len(deaths_by_state))\n",
    "ax.bar(x, deaths_by_state.values)\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(deaths_by_state.index, rotation=30, ha=\"right\")\n",
    "\n",
    "for i, v in enumerate(deaths_by_state.values):\n",
    "    ax.text(i, v, str(int(v)), ha=\"center\", va=\"bottom\")\n",
    "\n",
    "ax.set_title(\"Direct Fatalities by State\\nJuly 26â€“28, 2022\")\n",
    "ax.set_ylabel(\"Deaths\")\n",
    "\n",
    "ax.grid(True, axis=\"y\", linewidth=0.3)\n",
    "plt.tight_layout()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ef0876-98e7-49ee-a00c-9b076030df8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9123e644-0fa3-4f40-a944-ae7f1f5f8c91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NPL 2024b",
   "language": "python",
   "name": "npl-2024b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
